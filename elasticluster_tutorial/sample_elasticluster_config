[cloud/google]
provider=google
gce_client_id=<YOUR_GOOGLE_CLIENT_ID>
gce_client_secret=<YOUR_GOOGLE_CLIENT_SECRET>
gce_project_id=<YOUR_GOOGLE_PROJECT_ID>
noauth_local_webserver=True
zone=<YOUR_PREFERRED_GOOGLE_ZONE>

[login/google]
image_user=<YOUR_GOOGLE_USERNAME_OR_EMAIL>
image_user_sudo=root
image_sudo=True
user_key_name=elasticluster
user_key_private=~/.ssh/google_compute_engine
user_key_public=~/.ssh/google_compute_engine.pub

[setup/ansible]
ansible_forks=20
ansible_timeout=200

[setup/ansible-slurm]
provider=ansible
frontend_groups=slurm_master
compute_groups=slurm_worker

# allow restart of compute nodes
compute_var_allow_reboot=yes
global_var_slurm_taskplugin=task/cgroup
global_var_slurm_proctracktype=proctrack/cgroup
global_var_slurm_jobacctgathertype=jobacct_gather/cgroup

# Added by Mo to avoid "Shared connection to xxx.xxx.xxx.xxx closed" errors
# See https://www.tecmint.com/fix-shared-connection-to-x-x-xx-closed-ansible-error/
ansible_python_interpreter=/usr/bin/python3

[cluster/gce]
cloud=google
login=google
setup=ansible-slurm
security_group=default
image_id=ubuntu-1804-bionic-v20210315a
flavor=n1-standard-4
frontend_nodes=1
compute_nodes=4
ssh_to=frontend
boot_disk_type=pd-ssd
boot_disk_size=50

[cluster/gce/frontend]
boot_disk_size=100

##########

[cluster/gce-high-mem]
cloud=google
login=google
setup=ansible-slurm
security_group=default
image_id=ubuntu-1804-bionic-v20210315a
flavor=n1-standard-4
frontend_nodes=1
compute_nodes=2
ssh_to=frontend
boot_disk_type=pd-standard
boot_disk_size=100

[cluster/gce-high-mem/compute]
flavor=n2-highmem-4
boot_disk_size=50


############### For Assignment 2
[cluster/gce-gpu]
cloud=google
login=google
setup=ansible-slurm
security_group=default
image_id=ubuntu-1804-bionic-v20210315a
flavor=n1-standard-2
frontend_nodes=1
compute_nodes=2
ssh_to=frontend
boot_disk_type=pd-standard
boot_disk_size=100

[cluster/gce-gpu/compute]
flavor=n1-highmem-4
boot_disk_size=50
